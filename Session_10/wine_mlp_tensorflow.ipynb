{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science for Business - Multi-Layer Perceptron (MLP) on Wine Reviews with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize notebook\n",
    "Load required packages and set up workspace, e.g., initialize the random number generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mMrhkr83sqpt"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDzj8w5e7DaY"
   },
   "source": [
    "Check if we are running on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FhHhJiZD6yQG"
   },
   "outputs": [],
   "source": [
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 130.000+ wine reviews (Source: https://www.kaggle.com/datasets/zynicide/wine-reviews) written by professional sommeliers. Besides a text, it contains information about the wine, such as the country of origin, the variety, the winery, the price, and the number of points given by the reviewer. The goal is this notebook is to predict the number of points given by the reviewer based on the text of the review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data from CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv(\"https://raw.githubusercontent.com/olivermueller/ds4b-2024/refs/heads/main/Session_10/winemag-data-130k-v2.csv\")\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CureExnsIS-p"
   },
   "source": [
    "Split data into three sets: training, validation, and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "BBpPCg5zILlu"
   },
   "outputs": [],
   "source": [
    "training = corpus.iloc[0:80000,].sample(n=10000) # to speed up training\n",
    "validation = corpus.iloc[80000:100000,]\n",
    "test = corpus.iloc[100000:,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5Mygm5yIYnh"
   },
   "source": [
    "For each dataset, store features and targets in separate variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "18PF5d6ZIN-U"
   },
   "outputs": [],
   "source": [
    "train_corpus_features = training[[\"description\"]]\n",
    "train_corpus_target = training[[\"points\"]]\n",
    "val_corpus_features = validation[[\"description\"]]\n",
    "val_corpus_target = validation[[\"points\"]]\n",
    "test_corpus_features = test[[\"description\"]]\n",
    "test_corpus_target = test[[\"points\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvhh11WuIeyk"
   },
   "source": [
    "Next, we have to create [TensorFlow `Datasets`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) from the Pandas dataframes. The use of TensorFlow Datasets follows a common pattern:\n",
    "\n",
    "1.   Create a dataset from raw data (e.g., a Pandas dataframe, a CSV file, multiple text files).\n",
    "2.   Apply transformations to preprocess the data in the dataset (e.g., tokenize and vectorize the texts).\n",
    "3. Iterate over the dataset and process its elements. Iteration happens in a streaming fashion, so the full dataset does not need to fit into memory.\n",
    "\n",
    "Here, we use the `from_tensor_slices` constructor to create datasets from dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "QOOszgPrQVVw"
   },
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((tf.cast(train_corpus_features.values, tf.string),\n",
    "                                               tf.cast(train_corpus_target.values, tf.int32)))\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((tf.cast(val_corpus_features.values, tf.string),\n",
    "                                             tf.cast(val_corpus_target.values, tf.int32)))\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((tf.cast(test_corpus_features.values, tf.string),\n",
    "                                              tf.cast(test_corpus_target.values, tf.int32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dXbjrq6NJ1Qk"
   },
   "source": [
    "Display some stats and examples from the created datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j9LXPIY-XxDH"
   },
   "outputs": [],
   "source": [
    "for inputs, targets in train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"===\")\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uwh4TWJbVFY9"
   },
   "source": [
    "# Vectorize documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rM7gEBL8KHzg"
   },
   "source": [
    "We will now use [TensorFlow's `TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) function to transform raw texts into numerical vectors. To do this, we simply map words to integers (`output_mode = 'int'`). To get document representation of a fixed length, we limit the length of documents to 100 tokens (longer documents will be truncated, shorter documents will be padded with zeros). We also set a maximum vocabulary size of 10000 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "oyJW-JetIN2w"
   },
   "outputs": [],
   "source": [
    "max_tokens = 10000\n",
    "max_length = 100\n",
    "\n",
    "text_vectorization = TextVectorization(\n",
    "    max_tokens = max_tokens,\n",
    "    output_mode = \"int\",\n",
    "    output_sequence_length = max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4ElfvWtKj0Q"
   },
   "source": [
    "Some apects of the `TextVectorization` function (e.g., the size and contents of the vocabulary) have to be fit using training data, which can be done with the `adapt` function (which can only be applied to the features (x) of the dataset). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s6xCAUAOMdYH"
   },
   "outputs": [],
   "source": [
    "train_ds_features_only = train_ds.map(lambda x, y: x)\n",
    "text_vectorization.adapt(train_ds_features_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DkQu4KlNLftU"
   },
   "source": [
    "Show the vocabulary that our vectorizer knows after being fit to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dn0lt-7CRaCq"
   },
   "outputs": [],
   "source": [
    "text_vectorization.get_vocabulary()[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VjeZpApaLvyx"
   },
   "source": [
    "Now, we can apply our adapted `text_vectorization` function to all three datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "jHGn2peqMYuP"
   },
   "outputs": [],
   "source": [
    "vectorized_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls = 4)\n",
    "\n",
    "vectorized_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls = 4)\n",
    "\n",
    "vectorized_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_WzVu2WMEn-"
   },
   "source": [
    "Show results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0etTA2eAUB6G"
   },
   "outputs": [],
   "source": [
    "for inputs, targets in vectorized_train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"===\")\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `get_vocabulary()` to get the word behind a specific index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization.get_vocabulary()[1385]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9QH9PcrSa7J"
   },
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_K1l5L-MQ7z"
   },
   "source": [
    "We are now ready to specify a neural network and feed it with the vectroized datasets. For convenience, we define a custome function `get_model` which defines the network architecture, creates a model from it, and compiles this model (by defining, e.g., an otpimizer and loss function). Note that we have to somehow reduce the dimensionality of the output of the embedding layer (sequence_length*embedding_dim). Here, we simply perform a global average pooling (i.e., average each element of the embedding vector over all tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "ZMB14gBYSblz"
   },
   "outputs": [],
   "source": [
    "def get_model(hidden_dim=32):\n",
    "    inputs = keras.Input(shape=(max_length,), dtype=\"int64\")\n",
    "    embedded = layers.Embedding(input_dim=max_tokens, output_dim=300, mask_zero=True)(inputs)\n",
    "    hidden1 = layers.GlobalAveragePooling1D()(embedded)\n",
    "    hidden2 = layers.Dense(hidden_dim, activation = \"relu\")(hidden1)\n",
    "    outputs = layers.Dense(1, activation = \"linear\")(hidden2)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer = tf.optimizers.Adam(),\n",
    "                  loss = \"mean_absolute_error\",\n",
    "                  metrics = [\"mean_absolute_error\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWZtLzduT_sH"
   },
   "source": [
    "Instantiate model and show it's architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "02bcqmcvTIDW"
   },
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0W4SzaRqUExq"
   },
   "source": [
    "Fit model on training data and save best model to disk. We use the validation set to monitor how the performance of the model evolves during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aQZ8zbxbSzZb"
   },
   "outputs": [],
   "source": [
    "callbacks = [keras.callbacks.ModelCheckpoint(\"mlp.keras\", save_best_only=True)]\n",
    "\n",
    "history = model.fit(vectorized_train_ds.cache(),\n",
    "          validation_data = vectorized_val_ds.cache(),\n",
    "          epochs = 3,\n",
    "          batch_size = 128,\n",
    "          callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s7NOcSDY0Y6I"
   },
   "source": [
    "Plot the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BoHl0JaJ5fJm"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['mean_absolute_error'])\n",
    "plt.plot(history.history['val_mean_absolute_error'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJ347rt_Tx0G"
   },
   "source": [
    "# Make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKR36ArkT2Gq"
   },
   "source": [
    "Load best model from training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "zA_ON4yBTb2a"
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"mlp.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LRhBvwPT1q-"
   },
   "source": [
    "Make predictions on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yckI-BTWSTFm"
   },
   "outputs": [],
   "source": [
    "preds = model.predict(vectorized_test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMFi18uLT7tG"
   },
   "source": [
    "Calculate prediction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "afIwDkWYS_i4"
   },
   "outputs": [],
   "source": [
    "print(metrics.mean_absolute_error(test_corpus_target, preds))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1kHkaqxz9sVdaOC2i4FJVOOFW9cCiBkYu",
     "timestamp": 1636383829898
    }
   ]
  },
  "kernelspec": {
   "display_name": "prodok",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
